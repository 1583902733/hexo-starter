---
title: EM算法
date: 2020-03-01
tag: 
- 数据挖掘
- 机器学习
description: 最大期望算法（Expectation-maximization algorithm，又译为期望最大化算法），是在概率模型中寻找参数最大似然估计或者最大后验估计的算法，其中概率模型依赖于无法观测的隐性变量。EM算法甚至被称为上帝算法，可见其重要性，哈哈。
---

## EM算法

**最大期望算法**经过两个步骤交替进行计算：

**第一步**是计算期望（E），利用对隐藏变量的现有估计值，计算其最大似然估计值；

**第二步**是最大化（M），最大化在E步上求得的最大似然值来计算参数的值。M步上找到的参数估计值被用于下一个E步计算中，这个过程不断交替进行。

极大似然估计用一句话概括就是：知道结果，反推条件θ。

### 似然函数

​		在数理统计学中，似然函数是一种关于统计模型中的参数的函数，表示模型参数中的似然性。“似然性”与“或然性”或“概率”意思相近，都是指某种事件发生的可能性。而极大似然就相当于最大可能的意思。

​		比如你一位同学和一位猎人一起外出打猎，一只野兔从前方窜过。只听一声枪响，野兔应声到下，如果要你推测，这一发命中的子弹是谁打的？你就会想，只发一枪便打中，由于猎人命中的概率一般大于你那位同学命中的概率，从而推断出这一枪应该是猎人射中的。

​		这个例子所作的推断就体现了最大似然法的基本思想。

#### 极大似然函数的求解步骤

假定我们要从10万个人当中抽取100个人来做身高统计，那么抽到这100个人的概率就是(概率连乘)：

[![img](https://camo.githubusercontent.com/44ea6036b0f048a8a54c4966ae5235a92ca5059f/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f4c282535437468657461293d4c28785f312c2e2e2e2c785f6e2537432535437468657461293d25354370726f645f253742693d312537442535452537426e2537447028785f692537432535437468657461292c2535437468657461253543696e2535436f6d696e7573)](https://camo.githubusercontent.com/44ea6036b0f048a8a54c4966ae5235a92ca5059f/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f4c282535437468657461293d4c28785f312c2e2e2e2c785f6e2537432535437468657461293d25354370726f645f253742693d312537442535452537426e2537447028785f692537432535437468657461292c2535437468657461253543696e2535436f6d696e7573)

现在要求的就是这个 [![img](https://camo.githubusercontent.com/df0aa4fb9188137c820a84bde49a45b85a35c63f/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f2535437468657461)](https://camo.githubusercontent.com/df0aa4fb9188137c820a84bde49a45b85a35c63f/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f2535437468657461)值，也就是使得 [![img](https://camo.githubusercontent.com/35ed34b04b7dfa29cb455e6c082ba0feac6ca611/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f4c28253543746865746129)](https://camo.githubusercontent.com/35ed34b04b7dfa29cb455e6c082ba0feac6ca611/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f4c28253543746865746129)的概率最大化，那么这时的参数[![img](https://camo.githubusercontent.com/df0aa4fb9188137c820a84bde49a45b85a35c63f/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f2535437468657461)](https://camo.githubusercontent.com/df0aa4fb9188137c820a84bde49a45b85a35c63f/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f2535437468657461) 就是所求。

为了便于分析，我们可以定义对数似然函数，将其变成连加的形式：

[![img](https://camo.githubusercontent.com/e853e5013d09355702174e70b8e2be8781a99aab/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f48282535437468657461293d6c6e4c282535437468657461293d6c6e25354370726f645f253742693d312537442535452537426e2537447028785f692537432535437468657461293d25354373756d5f253742693d312537442535452537426e2537446c6e7028785f69253743253543746865746129)](https://camo.githubusercontent.com/e853e5013d09355702174e70b8e2be8781a99aab/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f48282535437468657461293d6c6e4c282535437468657461293d6c6e25354370726f645f253742693d312537442535452537426e2537447028785f692537432535437468657461293d25354373756d5f253742693d312537442535452537426e2537446c6e7028785f69253743253543746865746129)

​		对于求一个函数的极值，通过我们在本科所学的微积分知识，最直接的设想是求导，然后让导数为0，那么解这个方程得到的θ就是了（当然，前提是函数L(θ)连续可微）。但，如果θ是包含多个参数的向量那怎么处理呢？当然是求L(θ)对所有参数的偏导数，也就是梯度了，从而n个未知的参数，就有n个方程，方程组的解就是似然函数的极值点了，最终得到这n个参数的值。

求极大似然函数估计值的一般步骤：

1. 写出似然函数；
2. 对似然函数取对数，并整理；
3. 求导数，令导数为0，得到似然方程；
4. 解似然方程，得到的参数即为所求；

### **EM算法计算步骤：**

1. 随机初始化分布参数θ

2. E步，求Q函数，对于每一个i，计算根据上一次迭代的模型参数来计算出隐性变量的后验概率（其实就是隐性变量的期望），来作为隐藏变量的现估计值：

   [![img](https://camo.githubusercontent.com/eb333741f60276c772e401bdecd5154389339e35/687474703a2f2f7778312e73696e61696d672e636e2f6d773639302f30303633304465666c793167353762726d696a31776a3330376c3031637132732e6a7067)](https://camo.githubusercontent.com/eb333741f60276c772e401bdecd5154389339e35/687474703a2f2f7778312e73696e61696d672e636e2f6d773639302f30303633304465666c793167353762726d696a31776a3330376c3031637132732e6a7067)

3. M步，求使Q函数获得极大时的参数取值）将似然函数最大化以获得新的参数值

   [![img](https://camo.githubusercontent.com/4899540d7e489246dc876035d932dd9c45994aab/687474703a2f2f7778312e73696e61696d672e636e2f6d773639302f30303633304465666c793167353762737775736b316a333064653031733379682e6a7067)](https://camo.githubusercontent.com/4899540d7e489246dc876035d932dd9c45994aab/687474703a2f2f7778312e73696e61696d672e636e2f6d773639302f30303633304465666c793167353762737775736b316a333064653031733379682e6a7067)

4. 然后循环重复2、3步直到收敛。

   EM算法是保证收敛的，不过可能会收敛到局部最优解。

算法详解参考：[如何通俗的理解EM算法](https://blog.csdn.net/v_july_v/article/details/81708386)，[EM算法]([https://github.com/NLP-LOVE/ML-NLP/tree/master/Machine%20Learning/6.%20EM](https://github.com/NLP-LOVE/ML-NLP/tree/master/Machine Learning/6. EM))

## GMM:

[高斯混合模型的实现](http://www.codebelief.com/article/2017/11/gmm-em-algorithm-implementation-by-python/)



