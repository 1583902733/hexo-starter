---
title: PCA主成分分析和SVD奇异值分解
date: 2020-02-28
tag:
- 数据挖掘
- 机器学习
description: PCA（Principal Component Analysis） 是一种常见的数据分析方式，常用于高维数据的降维，可用于提取数据的主要特征分量。奇异值分解(Singular Value Decomposition，以下简称SVD)是在机器学习领域广泛应用的算法，它不光可以用于降维算法中的特征分解，还可以用于推荐系统，以及自然语言处理等领域，是很多机器学习算法的基石。
---

## PCA

### 算法步骤：

设有 m 条 n 维数据。

1. 将原始数据按列组成 n 行 m 列矩阵 X；
2. 将 X 的每一行进行零均值化，即减去这一行的均值；
3. 求出协方差矩阵 ![[公式]](https://www.zhihu.com/equation?tex=C%3D%5Cfrac%7B1%7D%7Bm%7DXX%5E%5Cmathsf%7BT%7D) ；
4. 求出协方差矩阵的特征值及对应的特征向量；
5. 将特征向量按对应特征值大小从上到下按行排列成矩阵，取前 k 行组成矩阵 P；
6. ![[公式]](https://www.zhihu.com/equation?tex=Y%3DPX) 即为降维到 k 维后的数据。

### 性质

1. **缓解维度灾难**：PCA 算法通过舍去一部分信息之后能使得样本的采样密度增大（因为维数降低了），这是缓解维度灾难的重要手段；
2. **降噪**：当数据受到噪声影响时，最小特征值对应的特征向量往往与噪声有关，将它们舍弃能在一定程度上起到降噪的效果；
3. **过拟合**：PCA 保留了主要信息，但这个主要信息只是针对训练集的，而且这个主要信息未必是重要信息。有可能舍弃了一些看似无用的信息，但是这些看似无用的信息恰好是重要信息，只是在训练集上没有很大的表现，所以 PCA 也可能加剧了过拟合；
4. **特征独立**：PCA 不仅将数据压缩到低维，它也使得降维之后的数据各特征相互独立；

**过程详解参考**：https://zhuanlan.zhihu.com/p/77151308

## SVD

### **性质**　

对于奇异值,它跟我们特征分解中的特征值类似，在奇异值矩阵中也是按照从大到小排列，而且奇异值的减少特别的快，在很多情况下，前10%甚至1%的奇异值的和就占了全部的奇异值之和的99%以上的比例。

也就是说，我们也可以用最大的k个的奇异值和对应的左右奇异向量来近似描述矩阵。

也就是说：

![img](https://pic3.zhimg.com/80/v2-6a5a4da69ea5c7450d016fd2a8c7c436_720w.jpg)

其中k要比n小很多，也就是一个大的矩阵A可以用三个小的矩阵 ![[公式]](https://www.zhihu.com/equation?tex=U_%7Bm%5Ctimes+k%7D%2C%5Csum_%7B%7D%5E%7B%7D%7B_%7Bk%5Ctimes+k%7D%7D%2CV_%7Bk%5Ctimes+n%7D%5E%7BT%7D) 来表示。如下图所示，现在我们的矩阵A只需要灰色的部分的三个小矩阵就可以近似描述了。

![img](https://pic3.zhimg.com/80/v2-4437f7678e8479bbc37fd965839259d2_720w.jpg)

由于这个重要的性质，SVD可以用于PCA降维，来做数据压缩和去噪。也可以用于推荐算法，将用户和喜好对应的矩阵做特征分解，进而得到隐含的用户需求来做推荐。同时也可以用于NLP中的算法，比如潜在语义索（LSI）。

算法详解参考：刘建平老师的回答 https://zhuanlan.zhihu.com/p/29846048

​		

​		这是两个不同的数学定义。结论：**特征值和特征向量是针对方阵**才有的，而**对任意形状的矩阵都可以做奇异值分解**。



