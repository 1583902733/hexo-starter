---
title: L1正则化和L2正则化
date: 2020-03-04
tag: 
- 机器学习
description: 机器学习算法之Lasso回归、Ridge回归和ElasticNet回归。L1正则化和L2正则化的特点及其应用场景。
---



### 公式

![](http://q5xxri682.bkt.clouddn.com/asg.png)

### **简单的理解正则化：**

1. 正则化的目的：防止过拟合
2. 正则化的本质：约束（限制）要优化的参数

关于第1点，过拟合指的是给定一堆数据，这堆数据带有噪声，利用模型去拟合这堆数据，可能会把噪声数据也给拟合了，这点很致命，一方面会造成模型比较复杂，另一方面，模型的泛化性能太差了，遇到了新的数据让你测试，你所得到的过拟合的模型，正确率是很差的。

关于第2点，本来解空间是全部区域，但通过正则化添加了一些约束，使得解空间变小了，甚至在个别正则化方式下，解变得稀疏了。这一点不得不提到一个图，相信我们都经常看到这个图，但貌似还没有一个特别清晰的解释，这里我尝试解释一下，图如下：

![img](https://www.biaodianfu.com/wp-content/uploads/2019/03/ridge-lasso.png)



### **L1正则（Lasso回归）**

特点：L1正则化就是在loss function后边所加正则项为L1范数，加上L1范数容易得到稀疏解（0比较多）。

应用场景：如果输入特征的维度很高，而且是稀疏线性关系的话， 考虑使用Lasso回归。或者是要在一堆特征里面找出主要的特征，那么L1正则更是首选了。

### **L2正则（Ridge回归）**

特点：L2正则化就是loss function后边所加正则项为L2范数的平方，加上L2正则相比于L1正则来说，得到的解比较平滑（不是稀疏），但是同样能够保证解中接近于0（但不是等于0，所以相对平滑）的维度比较多，降低模型的复杂度。

应用场景：只要数据线性相关，用LinearRegression拟合得不是很好，需要正则化，可以考虑使用岭回归。

### **ElasticNet回归**

应用场景：在发现用Lasso回归太过(太多特征被稀疏为0)，而岭回归也正则化得不够(回归系数衰减太慢)的时候，可以考虑使用ElasticNet回归来综合，得到比较好的结果。



### 参考：

https://blog.csdn.net/yuxeaotao/article/details/94648381

https://www.biaodianfu.com/ridge-lasso-elasticnet.html