---
title: 文本特征提取
date: 2020-02-22
tags: 
- 机器学习
- NLP
description: 一些文本特征提取的方法以及scikit-learn对应工具的使用，如CountVectorizer和TfidfTransformer等。
---

## 文本特征提取

### 话语表示

文本分析是机器学习算法的主要应用领域。 然而，原始数据，符号文字序列不能直接传递给算法，因为它们大多数要求具有固定长度的数字矩阵特征向量，而不是具有可变长度的原始文本文档。

为解决这个问题，scikit-learn提供了从文本内容中提取数字特征的最常见方法，即：

- **令牌化（tokenizing）** 对每个可能的词令牌分成字符串并赋予整数形的id，例如通过使用空格和标点符号作为令牌分隔符。
- **统计（counting）** 每个词令牌在文档中的出现次数。
- **标准化（normalizing）** 在大多数的文档 / 样本中，可以减少重要的次令牌的出现次数的权重。。

在该方案中，特征和样本定义如下：

- 每个**单独的令牌发生频率**（标准化或不标准化）被视为一个**特征**。
- 给定**文档**中所有的令牌频率向量被看做一个多元sample**样本**。

因此，**文本的集合可被表示为矩阵形式，每行对应一条文本，每列对应每个文本中出现的词令牌(如单个词)。**

我们称**向量化**是将文本文档集合转换为数字集合特征向量的普通方法。 这种特殊思想（令牌化，计数和归一化）被称为 **Bag of Words** 或 “Bag of n-grams” 模型。 文档由单词出现来描述，同时完全忽略文档中单词的相对位置信息。稀疏矩阵存储。



### 常见 Vectorizer 使用方法：

#### CountVectorizer：

 CountVectorizer 类会将文本中的词语转换为词频矩阵，例如矩阵中包含一个元素$a[i][j]$它表示j词在i类文本下的词频。它通过 fit_transform 函数计算各个词语出现的次数。

```python
>>> from sklearn.feature_extraction.text import CountVectorizer
>>> vectorizer = CountVectorizer()
>>> corpus = [
...     'This is the first document.',
...     'This is the second second document.',
...     'And the third one.',
...     'Is this the first document?',
... ]
>>> X = vectorizer.fit_transform(corpus)
>>> X                              
<4x9 sparse matrix of type '<... 'numpy.int64'>'
 with 19 stored elements in Compressed Sparse ... format>
```

默认配置通过提取**至少 2 个字母的单词**来对 string 进行分词。做这一步的函数可以显式地被调用:

```python
>>> analyze = vectorizer.build_analyzer()
>>> analyze("This is a text document to analyze.") == (
...     ['this', 'is', 'text', 'document', 'to', 'analyze'])
True
```

通过get_feature_names()可获取词袋中所有文本的关键字，通过 toarray()可看到词频矩阵的结果。

```python
>>> vectorizer.get_feature_names() == (
...     ['and', 'document', 'first', 'is', 'one',
...      'second', 'the', 'third', 'this'])
True

>>> X.toarray()           
array([[0, 1, 1, 1, 0, 0, 1, 0, 1],
 [0, 1, 0, 1, 0, 2, 1, 0, 1],
 [1, 0, 0, 0, 1, 0, 1, 1, 0],
 [0, 1, 1, 1, 0, 0, 1, 0, 1]]...)
```

从 feature 名称到 column index（列索引） 的逆映射存储在 `vocabulary_` 属性中:

```python
>>> vectorizer.vocabulary_.get('document')
1
```

#### Tf–idf 项加权

​		在一个大的文本语料库中，一些单词将出现很多次（例如 “the”, “a”, “is” ），因此对文档的实际内容没有什么有意义的信息。 如果我们将直接计数数据直接提供给分类器，那么这些频繁词组会掩盖住那些我们关注但很少出现的词。为了重新计算特征权重，并将其转化为适合分类器使用的浮点值，因此使用 tf-idf 变换是非常常见的。

Tf表示术语**频率**，而 tf-idf 表示术语**频率**乘以**转制文档频率**: $\text{tf-idf(t,d)}=\text{tf(t,d)} \times \text{idf(t)}$.

**转制文档频率**:$\text{idf}(t) = log{\frac{1 + n_d}{1+\text{df}(d,t)}} + 1$，其中 ![n_d](https://sklearn.apachecn.org/docs/0.21.3/img/6d8a1d709ef804f4629126d6e1c449f1.jpg) 是文档的总数，![\text{df}(d,t)](https://sklearn.apachecn.org/docs/0.21.3/img/ede6a26a443c24b8cea943a4a6f144f0.jpg) 是包含术语 ![t](https://sklearn.apachecn.org/docs/0.21.3/img/12b2c1da1f9041738fa7153efc651372.jpg) 的文档数。

然后，所得到的 tf-idf 向量通过欧几里得范数归一化：$v_{norm} = \frac{v}{||v||_2} = \frac{v}{\sqrt{v{_1}^2 + v{_2}^2 + \dots + v{_n}^2}}$

```python
>>> from sklearn.feature_extraction.text import TfidfTransformer
>>> transformer = TfidfTransformer(smooth_idf=False)
>>> counts = [[3, 0, 1],
...           [2, 0, 0],
...           [3, 0, 0],
...           [4, 0, 0],
...           [3, 2, 0],
...           [3, 0, 2]]
...
>>> tfidf = transformer.fit_transform(counts)
>>> tfidf                         
<6x3 sparse matrix of type '<... 'numpy.float64'>'
    with 9 stored elements in Compressed Sparse ... format>

>>> tfidf.toarray()                        
array([[0.81940995, 0.        , 0.57320793],
       [1.        , 0.        , 0.        ],
       [1.        , 0.        , 0.        ],
       [1.        , 0.        , 0.        ],
       [0.47330339, 0.88089948, 0.        ],
       [0.58149261, 0.        , 0.81355169]])
```

**TfidfTransformer**通常结合**CountVectorizer**一起使用，不过也可以使用**TfidfVectorizer**，它是前两者的结合体。

```python
>>> from sklearn.feature_extraction.text import TfidfVectorizer
>>> vectorizer = TfidfVectorizer()
>>> vectorizer.fit_transform(corpus)
...                                
<4x9 sparse matrix of type '<... 'numpy.float64'>'
 with 19 stored elements in Compressed Sparse ... format>
```

参考资料：

https://blog.csdn.net/m0_37324740/article/details/79411651

https://sklearn.apachecn.org/docs/0.21.3/39.html